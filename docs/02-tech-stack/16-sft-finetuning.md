# SFT（监督微调）实战经验分享（阿东玩AI）

## 📚 基础概念篇

### 1. 为什么需要模型微调？

#### 核心原因
通用大模型在特定领域或任务表现可能不佳，微调可以实现：

- **领域专业化**：深度内化专业知识与术语
- **任务适配**：针对特定任务优化表现
- **纠偏能力**：修正模型在特定场景的偏差
- **数据安全**：私有数据不出域，安全可控
- **成本效率**：比从头训练成本更低

#### 微调 vs 其他方案

**相比长文本处理**：
- 微调能深度内化知识，无需依赖实时检索
- 定制化更强，显著提升特定场景性能

**相比知识库检索**：
- 无需复杂的检索系统架构
- 响应速度更快，不依赖外部数据源
- 能深度内化专业知识与任务逻辑
- 数据安全可控，成本效率更高

#### 三阶段训练流程

预训练、SFT、RLHF是大模型的标配三阶段训练流程：

- **预训练阶段**：模型通过大规模无标注文本学习语言的基础规律，获得词汇理解、语法结构、世界知识等通用能力。但此时模型还不会按照人类指令执行任务。

- **SFT阶段**：通过有监督的指令-回答对训练，让模型学会理解和遵循人类指令，同时在特定领域（如代码、数学、医疗等）获得专业能力。

- **RLHF阶段**：通过人类反馈强化学习，让模型输出更符合人类价值观和偏好，减少有害、偏见或不当内容。

> 💡 这个流程解决了从"会说话"到"会聊天"再到"聊得好"的递进问题。

### 2. SFT和预训练的区别

- **预训练**：让大模型获得通用能力
- **SFT**：提升大模型在特定领域的能力

### 3. SFT和RLHF的区别

| 维度 | SFT | RLHF |
|------|-----|------|
| **目标** | 提升特定领域能力 | 与人类意图对齐，约束输出 |
| **学习方式** | 监督学习 | 强化学习 |
| **反馈类型** | 正向反馈 | 正向+负向反馈 |
| **优化粒度** | Token级别 | 句子级别 |
| **能力上限** | 受限于训练数据 | 可通过经验超越老师 |

### 4. SFT和RAG的区别

| 对比维度 | SFT | RAG |
|----------|-----|-----|
| **实现方式** | 监督训练数据微调 | 外挂知识库检索 |
| **知识获取** | 底层认知对齐 | 表层特征提取 |
| **适用场景** | 定制能力、低延迟 | 动态数据、高可解释性 |
| **实现难度** | 最高 | 中等 |
| **通用能力** | 可能下降 | 保持较好 |

### 5. SFT和增量预训练的区别

- **目的角度**：
  - SFT：激发特定领域或任务能力
  - Continue-pretrain：解决domain不匹配问题

- **流程角度**：Continue-pretrain → Pretrain → SFT

- **数据量角度**：增量预训练 >> SFT（99%情况下不使用增量预训练）

### 6. SFT和上下文学习的区别

**In-context Learning**：
- 通过少量示例激发模型能力（prompt工程）
- 不修改模型参数，无反向传播
- GPT-3论文首次提出

### 7. SFT和LoRA、PEFT的区别

- **全参数微调（SFT）**：
  - ✅ 精度上限更高
  - ❌ 资源需求高、易过拟合

- **PEFT方法**：
  - **LoRA**：低秩矩阵近似，省内存但精度略低
  - **Prefix Tuning**：添加可训练前缀嵌入
  - **Adapter Tuning**：插入小型神经网络

## 🔧 实践操作篇

### 8. 模型任务常见分类

#### 监督微调类型

**指令微调**：
- 明确任务指令，如翻译、摘要等
- 单轮任务导向，指令-输出格式

**对话微调**：
- 多轮对话数据训练
- 适用于客服、助手等场景

**领域适配**：
- 特定领域术语和知识
- 如医疗、法律、金融等专业领域

**文本分类**：
- 结构化标签数据
- 如情感分析、意图识别等

**模型推理微调**：
- 思维链标注数据
- 如数学解题、逻辑推理等

#### 其他微调类型

**知识蒸馏**：
- 将复杂模型知识迁移至轻量模型
- 降低推理成本，保持性能

**强化学习微调**：
- 结合人类反馈优化生成质量
- 提升安全性与风格一致性

**多模态微调**：
- 处理文本、图像、语音等跨模态数据
- 实现图文问答、视频分析等场景

#### 按参数更新方式分类

- **全参数微调**（大公司主流方案）
- **PEFT微调**（LoRA、Adapter等）
- **部分参数冻结微调**

### 9. 微调基本流程

微调的完整流程包括以下步骤：

1. **选定预训练模型**：选择适合的基座模型
2. **准备微调数据集**：构建高质量训练数据
3. **基线测试**：对微调前模型进行测试（用于后续对比）
4. **设定超参数**：配置学习率、批次大小等关键参数
5. **执行微调训练**：启动训练过程
6. **观测训练过程**：监控loss曲线和指标变化
7. **效果测试对比**：评估微调后模型性能
8. **迭代优化**：如效果不满意，调整数据集和超参数
9. **模型导出部署**：导出并部署满意的微调模型

### 10. SFT前提条件

**三大基础条件**：

1. **基座模型**：从ModelScope等平台下载
2. **微调数据**：格式、数量、质量
3. **微调环境**：硬件、软件、工具

**主要工具框架**：
- **LLaMA-Factory**：完善的微调工具平台
- **DeepSpeed**：分布式训练框架
- **Transformers**：HuggingFace开源库
- **Unsloth**：优化的微调工具

### 11. 基座模型选择策略

**模型类型选择**：
- **Base模型**：基础语言能力，无指令遵循
- **Chat模型**：已具备对话能力（推荐）
- **量化模型**：节省显存的压缩版本

**模型大小选择**：
- 单一任务：小模型(0.6B-7B)够用
- 复杂任务：大模型(70B+)效果更好

## 📊 数据构建篇

### 12. 微调数据集格式

#### 常见数据格式

模型微调数据集无明确格式要求，一般在代码中抹除差异，将其转为格式化字符串数组。主流格式有：

**Alpaca格式**（最常用）：
- 结构简洁，适用于单轮任务、指令微调
```json
{
  "instruction": "任务描述",
  "input": "具体输入", 
  "output": "期望输出"
}
```

**ShareGPT格式**（多轮对话）：
- 支持多轮对话与工具调用
```json
[
  {"from": "human", "value": "用户消息"},
  {"from": "gpt", "value": "助手回复"}
]
```

**COT格式**（推理任务）：
- Question：问题描述
- Thinking：思考过程
- Answer：最终答案

### 13. 数据集用途分类

#### 三类数据集构成学习闭环

**训练集、验证集、测试集**分别如日常练习题、模拟考试卷、最终期末考试，缺一不可：

- **训练集**：助模型学规律，如日常练习
- **验证集**：调策略防过拟合，如模拟考试
- **测试集**：验真实水平且需隔离，如期末考试

#### 数据划分策略

- **完整数据集**：包含三类数据，常按比例划分
- **数据不足时**：可用交叉验证等方法
- **时间序列数据**：按时间顺序划分，避免数据泄露

### 14. 训练数据集构建

#### 数据获取策略

1. **人工标注**：质量最高，成本最大
2. **模型生成+人工筛选**：GPT-4生成，人工质检
3. **现有数据改写**：格式转换和重组
4. **爬虫+清洗**：网络数据获取和过滤

#### 数据质量控制

- **一致性检查**：统一回答风格，避免矛盾
- **多样性保证**：丰富指令表达方式
- **噪声注入**：适当加入拼写错误提升鲁棒性
- **格式统一**：标准化JSON、markdown处理

#### 实践经验

> ⚠️ **关键要点**：
> - 数据质量 > 数据数量
> - 复杂推理任务中答案冲突会让模型"学疯"
> - Instructions字段必须明确角色定位
> - 建议小批量测试后再大规模生产

### 12. 数据量需求

**推荐范围**：2K-10W样本

**经典案例**：
- **LIMA论文**：约1万份高质量样本即可达到理想效果
- **InstructGPT**：微调阶段使用约1万组数据

> 💡 **核心理念**：重质量轻数量，精工细作胜过粗制滥造

### 13. 数据质量评估

#### 定量指标
- **覆盖度**：指令类型覆盖全面性
- **多样性**：n-gram重复率衡量
- **长度分布**：输入输出长度合理性
- **标签一致性**：同类任务标注统一度

#### 定性评估
- **答案质量**：准确、完整、有用
- **逻辑一致性**：前后不矛盾
- **角色一致性**：人设风格统一
- **安全性**：无有害偏见内容

#### 实用检查方法
1. **随机抽样**：每1000条抽查50-100条
2. **交叉验证**：多人标注计算一致性
3. **模型对比**：现有模型预测vs标准答案
4. **A/B测试**：不同质量数据效果对比

## 💻 技术实施篇

### 14. 硬件要求

#### 显存需求估算
**经验公式**：显存 ≈ 模型参数量 × 12倍

**示例**：1B模型 → 约12GB显存

#### 硬件兼容性
- **V100**：不支持Flash Attention和BF16
- **量化模型**：GPTQ需配合PEFT使用
- **LoRA微调**：训练和测试代码不同，需模型合并

### 15. 训练过程特征

#### Loss曲线规律
- **训练Loss**：先急剧下降，后趋于平缓
- **验证Loss**：先下降后上升（过拟合信号）

#### 过拟合现象
- **第2个epoch**：Loss突然急剧下降
- **原因**：大模型在第1个epoch已记住训练集

> 📈 **判断标准**：训练10个epoch仍学不会，说明模型能力不足

## ⚙️ 参数调优篇

### 16. 核心参数调优策略

#### 微调关键参数概述

模型训练关键超参数有三大核心：

> 💡 **参数关系**：
> - **训练轮数**：指遍历训练数据次数，少则不熟、多则过拟合
> - **学习率**：决定参数调整幅度，大则进步快易走偏，小则稳定但慢  
> - **批量大小**：每次更新用的样本数，大则快而粗、小则慢而细
> 
> 实际需调参验证，小白用默认即可。

#### 一、学习率 (Learning Rate)
- **推荐范围**：4e-5 到 5e-5
- **全参数微调**：1e-5（防止破坏原始知识）
- **LoRA微调**：5e-5（可用较大学习率）
- **数据集关系**：大数据集用大学习率
- **模型大小**：小模型(1-7B)用大学习率，大模型(70B+)用小学习率
- **调整原则**：决定参数调整幅度，大则进步快易走偏，小则稳定但慢

#### 二、训练轮数 (Epochs)
- **推荐范围**：3-10个epoch
- **数据量关系**：数据越大epoch越少
- **收敛判断**：未收敛增加epoch，过快收敛可提前停止
- **学习率衰减**：保持在0.5-1.5之间
- **核心原理**：指遍历训练数据次数，少则不熟、多则过拟合

#### 三、批量大小 (Batch Size)
- **显存影响**：批量越大显存占用越大
- **计算公式**：实际batch_size = per_device_batch_size × gradient_accumulation_steps × GPU数量
- **梯度累积**：推荐16/32/64/128
- **效果权衡**：大批量快速收敛但易过拟合
- **特点对比**：每次更新用的样本数，大则快而粗、小则慢而细

#### 四、截断长度 (Cutoff Length)
- **显存消耗**：每1024 token ≈ 2.5G显存
- **长度设定**：设为数据集最大长度
- **数据处理**：超长数据(>5%)建议训练前剔除

**长度检查工具**：
- LLaMA-Factory: `scripts/stat_utils/length_cdf.py`
- 在线工具: https://tiktokenizer.vercel.app/

#### 五、LoRA秩 (LoRA Rank)
- **推荐范围**：8-16
- **显存占用**：约2G
- **能力权衡**：小秩稳定，大秩适应复杂任务
- **调整策略**：模型没学会就调大秩

#### 六、验证集比例 (Validation Size)
- **小数据集**(<1000)：0.1-0.2，验证集≥100样本
- **大数据集**(>10000)：0.05-0.1，验证集≥1000样本
- **过拟合判断**：训练loss↓ 验证loss↑
- **正则化**：L1/L2正则化、Dropout

#### 七、显存优化策略
**显存构成**：模型权重 + 激活值 + 训练框架 + LoRA适配器

**优化方法**：
- **liger_kernel**：降低激活值内存占用
- **DeepSpeed Stage**：
  - Stage 0：简单快速，显存占用大
  - Stage 1/2/3：显存分摊，通信开销递增

## 📈 效果评估篇

### 17. 效果评估体系

#### 客观评估指标
- **Loss曲线**：训练收敛情况
- **困惑度(Perplexity)**：预测不确定性
- **BLEU/ROUGE**：与参考答案相似度
- **准确率/F1**：分类抽取任务精度

#### 主观评估维度
- **任务完成度**：指令理解执行能力
- **回答质量**：准确性、完整性、有用性
- **语言流畅性**：自然表达、逻辑清晰
- **角色一致性**：人格风格保持

#### 实用评估方法
1. **测试集验证**：高质量测试集定期评估
2. **人工评分**：多人评估计算一致性
3. **A/B对比**：与基线模型效果比较
4. **在线反馈**：真实用户使用反馈

#### 评估注意事项
- 关注实际应用效果，不只看训练指标
- 建立多维度评估体系
- 定期更新测试集避免过拟合
- 重视边界情况处理能力

## ⚠️ 风险防控篇

### 18. 不良后果及避免方法

#### 主要风险
1. **通用能力下降**：特定领域能力↑，通用能力↓
2. **过拟合现象**：模型记忆训练集，泛化能力差
3. **模型幻觉**：乱说话、上下文矛盾、事实错误

#### 解决方案
- **数据配比**：加入通用数据保持通用能力
- **PEFT方法**：降低过拟合风险
- **学习率调整**：防止过度拟合
- **正则化技术**：L1/L2、Dropout等

### 19. 推理耗时评估

**耗时公式**：`预测时间 = k×x + b`
- `b`：首个token耗时（与prompt长度正相关）
- `k`：后续每个token耗时
- `x`：生成token总数

> 💡 **实践启示**：COT效果好但耗时长，需在效果和效率间平衡

### 20. SFT Packing技术

**定义**：将多个SFT数据打包到一个样本内训练

**优点**：充分利用GPU算力，加快训练速度
**缺点**：不利于短文本和多轮对话
**建议**：一般情况下不推荐使用

## 🎯 核心原理篇

### 21. SFT原理总结

> **一句话概括**：
> - 预训练 = next token prediction的自监督学习
> - SFT = next token prediction的监督学习
> - 反馈粒度都是token级别

> **形象比喻**：SFT像背书，一般不存在学不会，只存在不会泛化

---

## 📝 总结

SFT作为大模型训练的关键环节，需要在数据质量、参数调优、效果评估等多个维度精心设计。成功的SFT项目往往遵循"数据为王、质量优先、持续迭代"的原则。希望这份经验分享能为大家的SFT实践提供有价值的参考！

> 🚀 **记住**：好的SFT不是一蹴而就的，需要在实践中不断调优和完善！