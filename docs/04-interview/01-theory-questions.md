# AI Agent 面试题库 - 理论基础篇

## 📚 适用对象
- ✅ **算法工程师**：必学,深入理解原理,能推导公式
- ✅ **开发工程师**:必学,理解基础概念,知道如何应用
- ⏱️ **建议学习时间**:算法岗 5天,开发岗 3天

## 📖 使用指南
- **学习建议**:先独立思考每个问题,尝试构建自己的答案,然后再对照参考思路查漏补缺
- **难度分级**:⭐基础 ⭐⭐进阶 ⭐⭐⭐高级
- **公司来源**:标注真题来源公司(字节/阿里/腾讯等)

---

## 第一部分:LLM 核心理论(32题)

### 1.1 Transformer 架构与注意力机制(必考⭐⭐⭐)

#### Q1:请详细解释一下 Transformer 模型中的自注意力机制是如何工作的?它为什么比 RNN 更适合处理长序列?

**难度**:⭐⭐
**岗位**:通用
**标签**:#Transformer #Attention #架构
**公司**:字节、阿里、腾讯(高频)

**核心考点**:
- Q/K/V 矩阵计算流程
- Attention 公式推导
- 为什么优于 RNN

**算法岗回答要点**:
1. **自注意力机制原理**
   - 输入序列通过三个线性变换得到 Q(Query)、K(Key)、V(Value)
   - 计算注意力分数:scores = QK^T / √d_k
   - Softmax 归一化得到注意力权重
   - 加权求和:output = softmax(scores) · V

2. **数学推导**
   ```
   Attention(Q,K,V) = softmax(QK^T/√d_k)V
   ```
   - 为什么除以√d_k?防止点积过大导致梯度消失
   - Multi-Head 机制:并行计算多个注意力头,捕获不同子空间的特征

3. **vs RNN 的优势**
   - **并行计算**:RNN 必须顺序计算,Transformer 可以并行处理整个序列
   - **长距离依赖**:RNN 存在梯度消失/爆炸,Transformer 通过直接注意力机制解决
   - **计算复杂度**:序列长度 n,RNN 为 O(n),Self-Attention 为 O(n²)但可并行

**开发岗回答要点**:
1. **理解注意力机制的作用**
   - 模型能自动关注序列中重要的部分
   - 类似于"加权平均",权重由模型学习得到

2. **工程实现要点**
   - 使用成熟框架(PyTorch/TensorFlow)内置的 Attention 层
   - 注意 Attention Mask 的使用(Padding mask、Causal mask)
   - 推理时可以使用 KV Cache 加速

3. **优化技巧**
   - Flash Attention:减少显存占用,加速计算
   - Multi-Query Attention(MQA):共享 K/V,降低显存

**延伸问题**:
- Multi-Head Attention 的作用是什么?
  - 答:类似CNN的多通道,不同head关注不同特征子空间
- Self-Attention vs Cross-Attention 的区别?
  - 答:Self-Attention 的 Q/K/V 来自同一序列;Cross-Attention 的 Q 来自一个序列,K/V 来自另一个序列(如 Encoder-Decoder)

**面试技巧**:
- 开场先说核心公式,展示理论功底
- 画图说明计算流程(Q/K/V 矩阵乘法)
- 主动提及优化技术(Flash Attention)加分

---

#### Q2:什么是位置编码?在 Transformer 中,为什么它是必需的?请列举至少两种实现方式。

**难度**:⭐⭐
**岗位**:通用
**标签**:#位置编码 #Transformer
**公司**:字节、阿里(高频)

**核心考点**:
- 为什么需要位置编码
- 绝对位置编码 vs 相对位置编码
- Sinusoidal vs Learned Positional Encoding

**标准答案**:

1. **为什么需要位置编码**
   - Transformer 的 Self-Attention 是**置换不变**的(permutation invariant)
   - 即打乱输入顺序,输出结果不变(仅注意力权重分布不同)
   - 但语言是有顺序的,"我爱你" ≠ "你爱我"
   - 因此需要显式注入位置信息

2. **两种主流实现方式**

**方式一:Sinusoidal Position Encoding(正弦位置编码)**
```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```
优点:
- 无需训练,泛化性好(可处理比训练序列更长的输入)
- 固定函数,相对位置关系恒定

缺点:
- 表达能力有限

**方式二:Learned Positional Embedding(可学习位置编码)**
```python
pos_embedding = nn.Embedding(max_seq_len, d_model)
```
优点:
- 更灵活,模型可以学习最优的位置表示

缺点:
- 无法处理超过 max_seq_len 的序列
- 需要额外参数

**算法岗加分项**:
- 讨论相对位置编码(ROPE、ALiBi)
- 分析不同位置编码对长文本建模的影响

**开发岗加分项**:
- 知道 BERT 用的是 Learned Embedding
- 知道 GPT 系列用的是 Learned Embedding
- 了解如何在代码中实现和使用

---

#### Q3:请你详细介绍ROPE,对比绝对位置编码它的优劣势分别是什么?

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#ROPE #旋转位置编码 #长文本
**公司**:字节(真题)

**核心考点**:
- ROPE(Rotary Position Embedding)原理
- 为什么适合长文本
- 与绝对位置编码的对比

**标准答案**:

1. **ROPE 核心思想**
   - 通过旋转矩阵在复数域对 Q 和 K 进行位置编码
   - 关键特性:**相对位置依赖**,即只有两个位置的相对距离影响注意力分数

2. **数学原理**(算法岗必须掌握)
```
q_m = (W_q · x_m) · e^(imθ)
k_n = (W_k · x_n) · e^(inθ)

attention_score = q_m · k_n^T
                = (W_q · x_m) · (W_k · x_n)^T · e^(i(m-n)θ)
```
核心:注意力分数只依赖于相对位置 (m-n),而非绝对位置 m 和 n

3. **优势**
   - **外推性好**:训练2k长度,推理可以扩展到16k+(配合NTK-Aware Scaling)
   - **相对位置感知**:符合语言的相对位置特性
   - **计算高效**:仅对 Q/K 进行旋转变换,无额外参数

4. **劣势**
   - 实现相对复杂(需要理解复数旋转)
   - 对某些任务(如位置敏感任务)效果可能不如绝对位置编码

**vs 绝对位置编码对比**:

| 维度 | 绝对位置编码(APE) | ROPE |
|------|------------------|------|
| 泛化性 | 超过训练长度性能下降 | 外推性强 |
| 参数量 | 需要额外参数(Learned Embedding) | 无额外参数 |
| 长文本 | 表现较差 | 表现优秀 |
| 应用 | BERT、GPT早期版本 | LLaMA、GPT-NeoX、Qwen |

**面试加分点**:
- 能推导 ROPE 的数学公式
- 知道 LLaMA、Qwen 等模型都采用 ROPE
- 了解 NTK-Aware ROPE Scaling(进一步扩展上下文)

---

#### Q4:你知道MHA,MQA,GQA的区别吗?详细解释一下。

**难度**:⭐⭐⭐
**岗位**:通用(开发岗也需了解)
**标签**:#Multi-Head Attention #MQA #GQA
**公司**:字节、阿里(真题)

**标准答案**:

这三者都是 Attention 机制的变体,核心区别在于 **K/V 的头数设计**。

**1. MHA (Multi-Head Attention) - 标准多头注意力**
- 每个头都有独立的 Q/K/V
- 参数量:heads × d_k × d_model × 3 (Q/K/V各一份)
- 显存占用:**最大**(推理时需要缓存所有 K/V)

**2. MQA (Multi-Query Attention) - 多查询注意力**
- **所有头共享同一组 K/V**,每个头只有独立的 Q
- 参数量:heads × d_k × d_model (Q) + d_k × d_model × 2 (共享K/V)
- 显存占用:**最小**(KV Cache 只需存储一份)
- 优点:推理速度快(KV Cache 小),适合推理部署
- 缺点:精度可能略有下降

**3. GQA (Grouped-Query Attention) - 分组查询注意力**
- **折中方案**:将heads分成G组,每组共享K/V
- 例如:8个head,分成2组,每组4个head共享一套K/V
- 参数量:介于 MHA 和 MQA 之间
- 精度 vs 速度的平衡点

**对比表格**:

| 类型 | K/V头数 | Q头数 | KV Cache | 精度 | 速度 | 代表模型 |
|------|---------|-------|----------|------|------|---------|
| **MHA** | H | H | 最大 | 最高 | 慢 | BERT、GPT-3 |
| **MQA** | 1 | H | 最小 | 略降 | 最快 | PaLM、Falcon |
| **GQA** | G (1<G<H) | H | 中等 | 平衡 | 平衡 | LLaMA-2、Mistral |

**算法岗深入理解**:
- MQA 为什么能work?理论上信息瓶颈在K/V,但实验表明共享K/V影响不大
- GQA 如何选择分组数 G?通常设为 H/4 或 H/8,兼顾精度和效率

**开发岗实际应用**:
- 推理场景优先选 MQA/GQA(减少显存,加速推理)
- 训练场景可以用 MHA(精度优先)
- LLaMA-2 70B使用 GQA,8个head分成2组

**面试技巧**:
- 画图说明三者区别(K/V头数)
- 提及 KV Cache 对推理的影响
- 举例说明哪些模型用了哪种方案

---

#### Q5:请比较一下几种常见的 LLM 架构,例如 Encoder-Only, Decoder-Only, 和 Encoder-Decoder,并说明它们各自最擅长的任务类型。

**难度**:⭐⭐
**岗位**:通用
**标签**:#模型架构 #Encoder-Decoder
**公司**:阿里、腾讯(高频)

**标准答案**:

**1. Encoder-Only (编码器架构)**
- **代表模型**:BERT、RoBERTa、ALBERT
- **Attention机制**:双向Attention(可以看到前后文)
- **预训练任务**:MLM(Masked Language Modeling)
- **擅长任务**:
  - ✅ 文本分类(情感分析、主题分类)
  - ✅ 序列标注(NER、词性标注)
  - ✅ 问答任务(抽取式QA)
  - ✅ 文本相似度计算
- **不擅长**:文本生成(因为是双向,无法自回归生成)

**2. Decoder-Only (解码器架构)**
- **代表模型**:GPT系列、LLaMA、Qwen
- **Attention机制**:单向Attention(Causal Attention,只能看到前文)
- **预训练任务**:CLM(Causal Language Modeling / Next Token Prediction)
- **擅长任务**:
  - ✅ 文本生成(续写、创作)
  - ✅ 对话任务(ChatGPT)
  - ✅ 代码生成(Codex)
  - ✅ In-Context Learning(少样本学习)
- **特点**:Scaling Law 效果最好,是目前大模型主流架构

**3. Encoder-Decoder (编码器-解码器架构)**
- **代表模型**:T5、BART、mT5
- **Attention机制**:
  - Encoder:双向Attention
  - Decoder:单向Attention + Cross-Attention(连接Encoder输出)
- **预训练任务**:Seq2Seq任务(如文本去噪、Span Masking)
- **擅长任务**:
  - ✅ 机器翻译
  - ✅ 文本摘要
  - ✅ 文本改写
  - ✅ 任何需要"理解输入+生成输出"的任务

**对比总结**:

| 架构 | Attention | 擅长任务 | 代表模型 | Scaling潜力 |
|------|-----------|---------|----------|-----------|
| **Encoder-Only** | 双向 | 理解类任务 | BERT | 中 |
| **Decoder-Only** | 单向(Causal) | 生成类任务 | GPT、LLaMA | **最高** |
| **Encoder-Decoder** | 混合 | Seq2Seq任务 | T5 | 中 |

**趋势洞察**(加分项):
- **当前主流**:Decoder-Only 一统天下(GPT、LLaMA、Qwen等)
- **原因**:
  1. Scaling Law 最好
  2. In-Context Learning 能力强
  3. 通过Prompt可以完成所有任务(包括理解类任务)
- **Encoder-Only 的未来**:在Embedding、检索等特定场景仍有价值

---

### 1.2 训练与优化(⭐⭐)

#### Q6:什么是Scaling Laws?它揭示了模型性能、计算量和数据量之间的什么关系?这对LLM的研发有什么指导意义?

**难度**:⭐⭐
**岗位**:算法岗重点
**标签**:#Scaling Law #模型训练
**公司**:字节、OpenAI(真题)

**标准答案**:

**1. Scaling Laws 定义**
- OpenAI 在2020年提出的经验定律
- 核心发现:模型性能与三个因素呈**幂律关系**:
  - N:模型参数量
  - D:训练数据量
  - C:计算量(FLOPs)

**2. 核心公式**(简化版)
```
Loss ∝ N^(-α) ∝ D^(-β) ∝ C^(-γ)
```
其中 α、β、γ 是经验系数(约0.05-0.1)

关键结论:
- 在固定计算预算下,应该**同时增大模型和数据**,而非只增大其中之一
- 最优配比:N ∝ D^0.5 (Chinchilla 论文修正)

**3. Chinchilla Scaling Laws(2022更新)**
DeepMind 的 Chinchilla 论文发现:
- **之前的模型训练不足**:参数增大,但数据量没跟上
- 最优配比:对于计算量 C,模型参数 N 和数据量 D 应该**等比例增长**
  ```
  N_optimal ≈ D_optimal ≈ C^0.5
  ```
- 实践:训练 70B 模型应该用 1.4T tokens,而非之前的300B tokens

**4. 对 LLM 研发的指导意义**

**意义一:资源分配**
- 不要盲目堆参数,数据质量同样重要
- Chinchilla(70B)用更多数据,超越 Gopher(280B)

**意义二:训练策略**
- **小模型充分训练** > 大模型欠训练
- LLaMA-2 7B 训练2T tokens,超越很多更大的模型

**意义三:成本优化**
- 给定算力预算,可以预估最优的N和D
- 避免浪费(要么参数太大数据不够,要么数据够但模型太小)

**意义四:性能预测**
- 可以根据小规模实验,外推预测大规模训练效果
- 指导决策:是否值得投入资源训练更大模型

**实际案例**:
- **LLaMA 系列**:基于 Scaling Laws,选择适中参数量(7B/13B/70B),用更多数据训练
- **Mistral 7B**:7B参数,超越13B甚至30B的模型,证明数据质量的重要性

**面试加分点**:
- 能区分 OpenAI Scaling Laws 和 Chinchilla Scaling Laws
- 知道 Chinchilla 的核心贡献:修正了 N 和 D 的最优比例
- 了解最新趋势:小模型+高质量数据(如Phi系列)

---

#### Q7:在LLM的推理阶段,有哪些常见的解码策略?请解释 Greedy Search, Beam Search, Top-K Sampling 和 Nucleus Sampling (Top-P) 的原理和优缺点。

**难度**:⭐⭐
**岗位**:通用
**标签**:#解码策略 #推理
**公司**:字节、阿里(高频)

**标准答案**:

LLM 每次生成一个 token,如何从词表中选择下一个token?这就是解码策略。

**1. Greedy Search (贪心搜索)**
- **原理**:每次选择概率最高的token
  ```python
  next_token = argmax(P(token | context))
  ```
- **优点**:
  - 简单、快速
  - 确定性(相同输入,输出一致)
- **缺点**:
  - 容易陷入重复
  - 缺乏多样性
  - 可能错过全局最优解
- **适用场景**:需要确定性输出(如代码生成、数学推理)

**2. Beam Search (束搜索)**
- **原理**:保留 k 个概率最高的候选序列
  ```
  每一步扩展k个候选,保留累积概率最高的k个
  最后选择总概率最高的序列
  ```
- **参数**:beam_size (通常2-10)
- **优点**:
  - 比Greedy更优(考虑全局)
  - 质量较高
- **缺点**:
  - 仍然偏向高频、保守的输出
  - 计算量是Greedy的k倍
  - 生成文本缺乏创造性
- **适用场景**:机器翻译、摘要(需要准确性)

**3. Top-K Sampling (Top-K采样)**
- **原理**:从概率最高的 K 个token中随机采样
  ```python
  # 过滤掉概率最低的token
  top_k_probs = sort(probs, descending=True)[:K]
  next_token = sample(top_k_probs)
  ```
- **参数**:K (通常20-100)
- **优点**:
  - 引入随机性,增加多样性
  - 避免采样到极低概率的token(质量保障)
- **缺点**:
  - K 是固定值,不够灵活
  - 概率分布陡峭时,K个token可能不够
  - 概率分布平缓时,K个token可能太多
- **适用场景**:需要一定多样性的生成任务

**4. Nucleus Sampling / Top-P Sampling (核采样)**
- **原理**:从累积概率达到 P 的最小token集合中采样
  ```python
  # 动态选择token数量
  sorted_probs = sort(probs, descending=True)
  cumsum_probs = cumsum(sorted_probs)
  nucleus = sorted_probs[cumsum_probs <= P]
  next_token = sample(nucleus)
  ```
- **参数**:P (通常0.9-0.95)
- **优点**:
  - **动态调整**采样范围(概率分布陡峭时采样少,平缓时采样多)
  - 平衡质量与多样性
  - 目前最主流的方法(GPT、LLaMA 默认)
- **缺点**:
  - 仍然是随机的,有时不可控
- **适用场景**:开放式文本生成(创作、对话)

**5. 组合策略**
实际应用中常常组合使用:
```python
# Top-P + Temperature
P(token) = softmax(logits / temperature)
# temperature > 1: 增加随机性
# temperature < 1: 更确定性
# temperature → 0: 接近Greedy
```

**对比总结**:

| 策略 | 确定性 | 多样性 | 质量 | 速度 | 适用场景 |
|------|--------|--------|------|------|---------|
| **Greedy** | 高 | 低 | 中 | 最快 | 代码生成、数学 |
| **Beam Search** | 高 | 低 | 高 | 慢 | 翻译、摘要 |
| **Top-K** | 低 | 中 | 中 | 快 | 通用生成 |
| **Top-P** | 低 | **高** | **高** | 快 | **对话、创作(主流)** |

**面试加分点**:
- 能解释 Top-P 为什么优于 Top-K(动态调整)
- 知道 Temperature 参数的作用
- 了解 ChatGPT 使用 Top-P + Temperature 策略

---

#### Q8:什么是词元化?请比较一下 BPE 和 WordPiece 这两种主流的子词切分算法。

**难度**:⭐⭐
**岗位**:通用
**标签**:#Tokenization #BPE #WordPiece
**公司**:字节、阿里(高频)

**标准答案**:

**1. 词元化(Tokenization)是什么?**
- 将文本切分成模型可以处理的最小单元(token)
- 桥梁:自然语言(连续字符) → 离散token → 数字ID → Embedding

**2. 为什么需要子词(Subword)切分?**
传统方法的问题:
- **字符级**:序列太长,训练慢,难以学习语义
- **词级**:
  - OOV问题(Out-of-Vocabulary 未登录词)
  - 词表过大(百万级)
  - 难以处理形态变化(run/running/runs)

**子词的优势**:
- ✅ 词表大小适中(3万-10万)
- ✅ 解决OOV(稀有词拆分成常见子词)
- ✅ 保留语义信息(比字符好)
- ✅ 处理形态变化(共享词根)

**3. BPE (Byte-Pair Encoding)**

**原理**:
1. 初始词表:所有单字符
2. 统计相邻token对的频率
3. 合并频率最高的token对 → 新token
4. 重复2-3步,直到词表达到目标大小

**示例**:
```
文本: "low low low lower lower newest newest newest newest"

迭代1: 合并频率最高的 "l" + "o" → "lo"
  → "low → "lo" "w"

迭代2: 合并 "lo" + "w" → "low"
  → "low" 作为一个token

迭代3: 合并 "low" + "e" → "lowe"
  → "lowe" "r"

最终词表: ["l", "o", "w", "e", "r", "s", "t", "n", "lo", "low", "lowe", "new", "newest", ...]
```

**编码过程**:
```
"lowest" → 查表:["low", "e", "st"] 或 ["lowe", "st"]
```

**特点**:
- ✅ 简单、高效
- ✅ 无需预定义词表
- ✅ 处理任意文本(包括稀有词、拼写错误)
- ❌ 对语言学知识利用不足

**4. WordPiece**

**原理**:
- 与BPE类似,但合并规则不同
- BPE:选择**频率最高**的token对
- WordPiece:选择使**语言模型困惑度下降最多**的token对

合并准则:
```
score(x, y) = P(xy) / (P(x) * P(y))
选择使 score 最大的 (x, y) 合并
```

**特点**:
- ✅ 基于语言模型,语义更合理
- ✅ Google 提出,BERT 使用
- ❌ 训练成本略高(需要语言模型)

**示例**:
```
WordPiece 切分: "playing" → ["play", "##ing"]
  ##表示非词首
```

**5. BPE vs WordPiece 对比**

| 维度 | BPE | WordPiece |
|------|-----|-----------|
| **合并规则** | 频率最高 | 语言模型困惑度 |
| **训练速度** | 快 | 慢(需训练LM) |
| **语义合理性** | 中 | 高 |
| **代表模型** | GPT、LLaMA、Qwen | BERT、T5 |
| **特殊标记** | 无 | ##(非词首标记) |

**6. 现代LLM的Tokenizer趋势**

- **SentencePiece**:BPE的改进版,支持多语言,无需预分词
  - 使用模型:LLaMA、Qwen、ChatGLM
  - 特点:将空格也作为token,支持任意语言

- **Byte-Level BPE**:GPT-2/3使用
  - 在字节级别运行,完全避免UNK
  - 256个字节作为基础词表

**面试加分点**:
- 能解释 BPE 的迭代合并过程
- 知道 BERT 用 WordPiece,GPT 用 BPE
- 了解 SentencePiece(现代LLM主流)
- 提及中文分词的特殊性(jieba vs字符级)

---

### 1.3 模型能力与现象(⭐⭐)

#### Q9:你觉得NLP和LLM最大的区别是什么?两者有何共同和不同之处?

**难度**:⭐
**岗位**:通用
**标签**:#NLP #LLM #范式转变
**公司**:字节、阿里(开放题)

**标准答案**:

这是一个很好的开放性问题,展示你对AI发展的理解。

**核心区别**:范式转变

**1. 传统NLP (Pre-LLM Era)**
- **范式**:任务驱动(Task-Specific)
  - 每个任务需要独立设计模型
  - 情感分类、NER、文本摘要都是不同的模型
- **数据需求**:每个任务需要大量标注数据
- **模型规模**:小(百万-千万参数)
- **能力边界**:只能做训练过的任务

**2. 大语言模型时代 (LLM Era)**
- **范式**:通用模型 + Prompt(Prompt-based)
  - 一个模型完成所有NLP任务
  - 通过改变Prompt即可切换任务
- **数据需求**:大量无标注数据预训练,少量或零样本适配
- **模型规模**:大(十亿-千亿参数)
- **能力边界**:涌现能力,可以做未训练过的任务

**对比表格**:

| 维度 | 传统NLP | LLM |
|------|---------|-----|
| **核心范式** | 任务特定模型 | 通用模型+Prompt |
| **数据需求** | 大量标注数据 | 海量无标注数据 |
| **模型规模** | 小(1M-100M参数) | 大(1B-1000B参数) |
| **训练方式** | 有监督学习 | 自监督预训练+微调/ICL |
| **泛化能力** | 弱(仅限训练任务) | 强(零样本/少样本泛化) |
| **涌现能力** | 无 | 有(推理、规划、工具使用) |
| **应用方式** | 集成多个专用模型 | 一个模型+不同Prompt |

**本质不同:理解 vs 生成**

传统NLP:
- 侧重**理解**任务(分类、标注、抽取)
- 编码器架构(BERT)为主

LLM:
- 侧重**生成**任务(续写、对话、创作)
- 解码器架构(GPT)为主
- 生成能力带来涌现能力

**相同之处**:
- 都基于Transformer架构
- 都需要大量数据训练
- 都依赖Self-Attention机制
- 都利用预训练+微调范式(虽然LLM更多用ICL)

**面试加分点**:
- 提及 "范式转变":从 Task-Specific → Foundation Model
- 讨论 "涌现能力":Scaling Law带来的质变
- 展望未来:LLM + 传统NLP的结合(如LLM+检索、LLM+知识图谱)

---

#### Q10:L1和L2正则化分别是什么,什么场景适合使用呢?

**难度**:⭐
**岗位**:算法岗
**标签**:#正则化 #机器学习基础
**公司**:阿里、腾讯(基础题)

**标准答案**:

正则化是防止过拟合的重要技术。

**1. L1 正则化(Lasso)**
```
Loss = MSE(y, ŷ) + λ Σ|w_i|
```
特点:
- 绝对值惩罚
- **稀疏性**:倾向于将不重要的权重压缩到0
- 效果:**特征选择**

**2. L2 正则化(Ridge)**
```
Loss = MSE(y, ŷ) + λ Σ(w_i)²
```
特点:
- 平方惩罚
- 权重均匀缩小,不会变成0
- 效果:**权重衰减**,防止某些权重过大

**对比**:

| 维度 | L1正则化 | L2正则化 |
|------|---------|---------|
| **公式** | λΣ\|w\| | λΣw² |
| **效果** | 稀疏权重(部分为0) | 权重衰减(接近0但不为0) |
| **导数** | 不可导(w=0处) | 可导 |
| **应用** | 特征选择、压缩模型 | 防止过拟合 |

**使用场景**:

**L1正则化适合**:
- 特征维度很高,需要特征选择
- 希望模型更可解释(只保留重要特征)
- 模型压缩、剪枝

**L2正则化适合**:
- **深度学习**(几乎是标配)
- 特征都比较重要,不希望完全舍弃
- 希望权重整体较小

**深度学习中的应用**:
- **Weight Decay**(权重衰减)本质就是L2正则化
- AdamW优化器:将权重衰减与梯度解耦
```python
w = w - lr * grad - lr * lambda * w  # Weight Decay
```

**面试加分点**:
- 能推导L1导致稀疏性的原因(菱形vs圆形等高线)
- 知道深度学习中Weight Decay的作用
- 了解Elastic Net(L1+L2结合)

---

#### Q11:"涌现能力"是大型模型中一个备受关注的现象,请问你如何理解这个概念?它通常在模型规模达到什么程度时出现?

**难度**:⭐⭐
**岗位**:通用
**标签**:#涌现能力 #Scaling Law
**公司**:字节、OpenAI(高频)

**标准答案**:

**1. 涌现能力(Emergent Abilities)定义**
- 当模型参数量达到一定规模时,**突然出现**之前小模型不具备的能力
- 这些能力不是通过显式训练获得的,而是自然"涌现"的
- 关键特征:**规模驱动的质变**

**2. 典型的涌现能力**

**能力一:In-Context Learning (上下文学习)**
- 小模型(<1B):无法通过示例学习
- 大模型(>10B):给几个示例,就能学会新任务
- 示例:
  ```
  英译中:
  Apple → 苹果
  Banana → 香蕉
  Orange → ?

  大模型能推理出: 橙子
  ```

**能力二:Chain-of-Thought Reasoning (思维链推理)**
- 小模型:直接给答案,常常错误
- 大模型(>100B):能够一步步推理
- 示例:
  ```
  问:商店有23个苹果,卖出17个,又进了5个,现在有几个?
  小模型:11 (错误)
  大模型:让我一步步计算:
    1. 最初:23个
    2. 卖出17个:23-17=6个
    3. 又进5个:6+5=11个
    答案:11个
  ```

**能力三:指令遵循(Instruction Following)**
- 小模型:难以准确理解复杂指令
- 大模型:能精确执行多步骤、条件性指令
- 示例:ChatGPT的复杂指令执行能力

**3. 涌现的规模阈值**

不同能力的出现规模不同:

| 涌现能力 | 出现规模(参数量) | 代表模型 |
|----------|-----------------|---------|
| **基础In-Context Learning** | ~10B | GPT-3(13B) |
| **思维链推理(CoT)** | ~100B | GPT-3(175B) |
| **复杂推理、规划** | ~500B | GPT-4(推测1.7T) |

**关键观察**:
- 并非线性增长,而是**突然出现**(类似相变)
- 在10B以下几乎看不到涌现能力
- 在100B以上涌现能力显著

**4. 为什么会涌现?**

目前理论解释:
- **假说一**:数据与参数的协同效应
  - 小模型:记忆能力有限,只能学习表面模式
  - 大模型:能学习深层结构、抽象概念

- **假说二**:Scaling Law的临界点
  - 某些能力需要跨越"理解鸿沟"
  - 只有足够大的模型才能跨越

- **假说三**:量变引起质变
  - 类似神经科学的"突触密度阈值"

**5. 争议与反思**

**支持观点**:
- GPT-3→GPT-4的能力飞跃证明了涌现
- 数学推理、代码生成能力的突然出现

**质疑观点**(Schaeffer et al. 2023):
- "涌现"可能是评估指标的问题
- 改用连续指标,可能看到平滑增长而非突变
- 部分"涌现"可能是数据污染导致

**面试加分点**:
- 能列举3-5个具体的涌现能力
- 知道涌现的规模阈值(10B/100B)
- 了解学术界对"涌现"的争议
- 提及Scaling Law与涌现的关系

---

#### Q12:激活函数有了解吗,你知道哪些LLM常用的激活函数?为什么选用它?

**难度**:⭐⭐
**岗位**:算法岗重点
**标签**:#激活函数 #模型架构
**公司**:字节、阿里(高频)

**标准答案**:

激活函数在LLM中主要用于FFN(Feed-Forward Network)层。

**1. Transformer FFN结构**
```python
FFN(x) = activation(x W1 + b1) W2 + b2
```

**2. LLM中常用的激活函数**

**函数一:GELU (Gaussian Error Linear Unit)**
```
GELU(x) = x * Φ(x)
Φ(x) = 标准正态分布的累积分布函数
```
近似公式:
```
GELU(x) ≈ 0.5x(1 + tanh(√(2/π)(x + 0.044715x³)))
```

**特点**:
- ✅ 平滑、可导
- ✅ 非单调性(在x<0时也有非零输出)
- ✅ 更好的梯度传播
- **使用模型**:BERT、GPT-2、GPT-3

**为什么选GELU?**
- 实验表明比ReLU效果好
- 引入随机性正则化(类似Dropout效果)
- 平滑性有助于优化

**函数二:SwiGLU (Swish + GLU)**
```
Swish(x) = x * sigmoid(x)
SwiGLU(x) = Swish(x) * (x W1) ⊙ (x W2)
```

**特点**:
- ✅ Google提出,实验效果最好
- ✅ 引入门控机制(GLU)
- **使用模型**:PaLM、LLaMA、Qwen

**为什么选SwiGLU?**
- 在大规模实验中,SwiGLU > GELU > ReLU
- GLU门控机制增强表达能力
- LLaMA论文验证:SwiGLU略优于GELU

**函数三:GeGLU (GELU + GLU)**
```
GeGLU(x) = GELU(x W1) ⊙ (x W2)
```

**特点**:
- GELU与GLU的结合
- **使用模型**:T5

**3. 对比表格**

| 激活函数 | 公式 | 特点 | 代表模型 | 复杂度 |
|---------|------|------|---------|-------|
| **ReLU** | max(0,x) | 简单、快速 | 早期Transformer | 低 |
| **GELU** | x·Φ(x) | 平滑、效果好 | BERT、GPT-2/3 | 中 |
| **Swish** | x·sigmoid(x) | 自门控 | 部分模型 | 中 |
| **SwiGLU** | Swish⊙Gate | 门控、最优 | **LLaMA、Qwen** | 高 |
| **GeGLU** | GELU⊙Gate | GELU+门控 | T5 | 高 |

**4. 趋势分析**

**早期(2017-2019)**:ReLU、GELU
- BERT:GELU
- GPT-2:GELU

**现代(2020-至今)**:SwiGLU为主
- PaLM:SwiGLU
- LLaMA:SwiGLU
- Qwen:SwiGLU
- Mistral:SwiGLU

**为什么SwiGLU成为主流?**
1. Google PaLM论文大规模实验验证效果最好
2. LLaMA开源,带动社区采用
3. 门控机制(GLU)理论上更强

**5. FFN的演进**

**标准FFN**:
```python
FFN(x) = GELU(xW1)W2
```

**GLU-based FFN**(参数量增加50%):
```python
FFN(x) = (Swish(xW1) ⊙ xW2) W3
```

**面试加分点**:
- 能解释 GELU 的公式和直觉
- 知道 LLaMA、Qwen 使用 SwiGLU
- 了解 GLU(Gated Linear Unit)机制
- 提及 SwiGLU vs GELU 的性能对比

---

#### Q13:混合专家模型（MoE）是如何在不显著增加推理成本的情况下，有效扩大模型参数规模的？请简述其工作原理。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#MoE #模型架构 #稀疏激活
**公司**:字节、DeepMind(高频)

---

#### Q14:在训练一个百或千亿参数级别的 LLM 时，你会面临哪些主要的工程和算法挑战？（例如：显存、通信、训练不稳定性等）

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#分布式训练 #工程挑战
**公司**:字节、阿里、腾讯(高频)

---

#### Q15:开源框架了解过哪些？Qwen，Deepseek的论文是否有研读过，说一下其中的创新点主要体现在哪？

**难度**:⭐⭐
**岗位**:通用
**标签**:#开源模型 #技术报告
**公司**:阿里、字节(常考)

---

#### Q16:最近读过哪些LLM比较前沿的论文，聊一下它的相关方法，针对什么问题，提出了什么方法，对比实验有哪些？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#前沿论文 #研究方向
**公司**:所有公司(开放题)

---

## 第二部分:VLM 多模态(11题)

### 2.1 核心概念与挑战(⭐⭐⭐)

#### Q1:多模态大模型(如 VLM)的核心挑战是什么?即如何实现不同模态信息(如视觉和语言)的有效对齐和融合?

**难度**:⭐⭐
**岗位**:算法岗、多模态方向
**标签**:#VLM #多模态对齐
**公司**:字节、阿里(多模态岗高频)

**标准答案**:

VLM(Vision-Language Model)的核心挑战:**异构模态的语义对齐**

**1. 核心挑战**

**挑战一:模态异构性**
- 视觉:连续、高维、空间结构(图像:H×W×C)
- 语言:离散、序列、符号表达(文本:token序列)
- 如何将两者映射到同一语义空间?

**挑战二:语义鸿沟(Semantic Gap)**
- 同一概念在不同模态的表达差异巨大
- 例如:"一只猫"(文本) vs 猫的图片(视觉)
  - 图片包含颜色、姿态、背景等信息
  - 文本只有符号"猫"
- 如何建立对应关系?

**挑战三:粒度不匹配**
- 图片是整体
- 文本可以描述局部("猫的耳朵")、整体("一只猫")、抽象("可爱")
- 如何建立不同粒度的对齐?

**2. 主流解决方案**

**方案一:对比学习(Contrastive Learning)- CLIP**
```
核心思想:拉近匹配的图文对,推远不匹配的

Loss = -log( exp(sim(img, text+)) / Σ exp(sim(img, text_i)) )
```
优点:
- 无需细粒度标注,只需图文对
- 大规模数据训练(4亿图文对)
- 强大的零样本能力

代表:CLIP、ALIGN

**方案二:跨模态注意力(Cross-Modal Attention)**
```
Visual Tokens → Cross-Attention → Language Model
```
机制:
- 图像编码成视觉token序列
- 语言模型通过Cross-Attention关注相关视觉区域
- 实现细粒度对齐

代表:Flamingo、BLIP-2

**方案三:统一多模态预训练**
```
[IMG] + [TEXT] → 统一Transformer → 联合表示
```
- 将图像和文本视为同等的token序列
- 在统一空间中训练
- 难点:计算量大

代表:BEiT-3、CoCa

**3. 对齐策略对比**


| 策略 | 对齐方式 | 优点 | 缺点 | 代表模型 |
| 策略 | 对齐方式 | 优点 | 缺点 | 代表模型 |
|------|---------|------|------|---------|
| **对比学习** | 全局匹配 | 简单、高效、零样本 | 粗粒度 | CLIP |
| **跨模态注意力** | 细粒度交互 | 精细对齐 | 计算量大 | Flamingo |
| **统一预训练** | 统一表示空间 | 理论最优 | 训练成本极高 | BEiT-3 |


**面试加分点**:
- 能清晰解释"语义鸿沟"问题
- 知道CLIP的对比学习原理
- 了解最新的VLM架构(LLaVA、Qwen-VL)

---

#### Q2:请解释 CLIP 模型的工作原理。它是如何通过对比学习来连接图像和文本的？

**难度**:⭐⭐⭐
**岗位**:算法岗、多模态方向
**标签**:#CLIP #对比学习
**公司**:字节、阿里(高频)

---

#### Q3:像 LLaVA 或 MiniGPT-4 这样的模型是如何将一个预训练好的视觉编码器（Vision Encoder）和一个大语言模型（LLM）连接起来的？请描述其关键的架构设计。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#VLM架构 #模态连接
**公司**:字节、阿里(高频)

---

#### Q4:什么是视觉指令微调？为什么说它是让 VLM 具备良好对话和指令遵循能力的关键步骤？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#视觉指令微调 #VLM训练
**公司**:字节、阿里

---

#### Q5:在处理视频等多模态数据时，相比于静态图片，VLM 需要额外解决哪些问题？（例如，如何表征时序信息？）

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#视频理解 #时序建模
**公司**:字节、腾讯

---

#### Q6:请解释Grounding在 VLM 领域中的含义。我们如何评估一个 VLM 是否能将文本描述准确地对应到图片中的特定区域？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#Grounding #视觉定位
**公司**:字节、阿里

---

#### Q7:请对比至少不同的 VLM 架构范式（如共享编码器 vs. 跨模态注意力融合），并分析它们的优劣。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#VLM架构 #架构对比
**公司**:字节、阿里

---

#### Q8:在 VLM 的应用中，如何处理高分辨率的输入图像？这会带来哪些计算和模型设计上的挑战？

**难度**:⭐⭐⭐
**岗位**:算法岗、工程优化
**标签**:#高分辨率 #计算优化
**公司**:字节、腾讯

---

#### Q9:VLM 在生成内容时，同样会遇到"幻觉"（Hallucination）问题，但它的表现形式和纯文本 LLM 有何不同？请举例说明。

**难度**:⭐⭐
**岗位**:通用
**标签**:#幻觉问题 #VLM缺陷
**公司**:字节、阿里、腾讯

---

#### Q10:除了图片描述和视觉问答（VQA），你还能列举出 VLM 的哪些前沿或具有潜力的应用方向？

**难度**:⭐⭐
**岗位**:通用
**标签**:#VLM应用 #前沿方向
**公司**:所有公司

---

#### Q11:有没有做过VLM相关方面的微调？什么模型？

**难度**:⭐
**岗位**:通用(项目经验)
**标签**:#VLM微调 #项目经验
**公司**:所有公司

---

### 2.2 多模态训练与优化(⭐⭐⭐)

#### Q12:多模态学习中常见的融合方式有哪些？早期融合 vs 晚期融合 vs 中间融合的区别和适用场景？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#多模态融合 #融合策略
**公司**:字节(真题)

---

#### Q13:Vision Transformer (ViT) 和 CNN 在图像特征提取上的优劣对比？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#ViT #CNN对比
**公司**:字节、阿里

---

#### Q14:什么是对比学习(Contrastive Learning)？InfoNCE loss 的公式和作用？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#对比学习 #InfoNCE
**公司**:字节(高频)

---

#### Q15:大模型训练中常用的优化器有哪些？AdamW 和 Adam 的区别是什么？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#优化器 #AdamW
**公司**:字节、阿里

---

#### Q16:如何评估多模态模型的性能？除了准确率，还有哪些指标？（如 Recall@K, mAP 等）

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#评估指标 #多模态评估
**公司**:字节(真题)

---

#### Q17:什么是 instruction tuning？在多模态场景下如何做？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#指令微调 #多模态训练
**公司**:字节

---

#### Q18:BLIP / BLIP-2 的核心创新点是什么？和 Flamingo 有什么区别？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#BLIP #VLM模型
**公司**:字节(前沿)

---

## 第三部分:RLHF 对齐技术(13题)

### 3.1 RLHF 核心流程(⭐⭐⭐)

#### Q1:和传统SFT相比,RLHF旨在解决语言模型中的哪些核心问题?为什么说SFT本身不足以实现我们期望的"对齐"目标?

**难度**:⭐⭐
**岗位**:算法岗重点
**标签**:#RLHF #模型对齐
**公司**:OpenAI、DeepMind、字节(高频)

**标准答案**:

**1. SFT的局限性**

**局限一:只能学习"what to say",不能学习"how good"**
- SFT:给定(指令,回答)对,模型学习模仿
- 问题:无法区分"好回答"和"更好的回答"
- 例如:
  ```
  指令:解释量子力学
  回答A:量子力学是...  (正确但平庸)
  回答B:量子力学是...  (深入且有趣)

  SFT:两者无差别,只要都是正确的
  RLHF:能学习到B更好
  ```

**局限二:数据覆盖不足**
- SFT需要大量高质量(指令,回答)对
- 但人类标注成本高,数据量有限
- 难以覆盖所有场景

**局限三:无法处理多样性偏好**
- 不同人对"好回答"的标准不同
- SFT只能学习单一风格
- RLHF可以学习人类偏好分布

**2. RLHF解决的核心问题**

**问题一:对齐人类偏好**
- SFT:对齐人类示范(behavior cloning)
- RLHF:对齐人类偏好(preference learning)
- 偏好数据更易获取(只需比较,无需写答案)

**问题二:处理主观性**
- 对于开放性问题(创作、对话),没有唯一正确答案
- RLHF通过奖励模型捕捉人类偏好的分布

**问题三:在线优化**
- SFT:离线学习,训练后不再改进
- RLHF:模型生成→人类评价→模型改进(闭环)

**3. RLHF的三阶段流程**

```
阶段1:SFT (Supervised Fine-Tuning)
  预训练模型 + 高质量指令数据 → SFT模型

阶段2:训练奖励模型 (Reward Model Training)
  收集人类偏好数据(A vs B,选哪个更好)
  → 训练Reward Model

阶段3:强化学习优化 (PPO)
  SFT模型 生成回答 → Reward Model评分
  → PPO算法优化 → 对齐模型
```

**4. 为什么SFT不够?**

**理论角度**:
- SFT是**行为克隆**(Behavior Cloning),只学习表面行为
- RLHF是**奖励建模**(Reward Modeling),学习内在价值
- 类比:
  - SFT:看视频学跳舞,只能模仿动作
  - RLHF:教练打分指导,理解"好"的标准

**实践角度**:
- InstructGPT实验:
  - 纯SFT:能遵循指令,但回答质量不稳定
  - SFT+RLHF:回答质量显著提升,更符合人类偏好

**5. SFT vs RLHF 对比**

| 维度 | SFT | RLHF |
|------|-----|------|
| **学习目标** | 模仿人类示范 | 优化人类偏好 |
| **数据需求** | 高质量(指令,回答)对 | 人类偏好对比数据 |
| **数据成本** | 高(需专家写答案) | 中(只需比较) |
| **覆盖范围** | 有限(数据覆盖) | 更广(泛化到相似场景) |
| **主观任务** | 差(无法学偏好) | 好(显式建模偏好) |
| **代表模型** | Alpaca、Vicuna | ChatGPT、Claude |

**面试加分点**:
- 能解释"对齐"的含义(Alignment)
- 知道 InstructGPT 论文的核心贡献
- 了解 RLHF 的三阶段流程
- 提及 RLHF 的局限(成本高、不稳定)

---

#### Q2:请详细阐述经典RLHF流程的三个核心阶段。在每个阶段，输入是什么，输出是什么，以及该阶段的关键目标是什么？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#RLHF流程 #三阶段
**公司**:OpenAI、字节、阿里(高频)

---

#### Q3:在RM训练阶段，我们通常收集的是成对比较数据，而不是让人类标注者直接给回复打一个绝对分数。你认为这样做的主要优势和潜在的劣势分别是什么？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#奖励模型 #数据标注
**公司**:OpenAI、字节

---

#### Q4:奖励模型的设计至关重要。它的模型架构通常如何选择？它与我们最终要优化的LLM是什么关系？在训练奖励模型时，常用的损失函数是什么？请解释其背后的数学原理（例如，可以结合Bradley-Terry模型来解释）。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#奖励模型 #Bradley-Terry
**公司**:OpenAI、DeepMind(高频)

---

#### Q5:在RLHF的第三阶段，PPO是最主流的强化学习算法。为什么选择PPO，而不是其他更简单的策略梯度算法（如REINFORCE）或者Q-learning系算法？PPO中的KL散度惩罚项起到了什么关键作用？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#PPO #强化学习
**公司**:OpenAI、DeepMind、字节(高频)

---

#### Q6:如果在PPO训练过程中，KL散度惩罚项的系数 β 设置得过大或过小，分别会导致什么样的问题？你将如何通过实验和观察来调整这个超参数？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#PPO #超参数调优
**公司**:OpenAI、字节

---

#### Q7:什么是"奖励作弊/奖励黑客"（Reward Hacking）？请结合一个具体的LLM应用场景给出一个例子，并探讨几种可能的缓解策略。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#Reward Hacking #安全对齐
**公司**:OpenAI、Anthropic、字节(重要)

---

#### Q8:RLHF流程复杂且不稳定。近年来出现了一些替代方案，例如DPO。请解释DPO的核心思想，并比较它与传统RLHF（基于PPO）的主要区别和优势。

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#DPO #RLHF替代
**公司**:字节、阿里(高频)

---

#### Q9:想象一下，你训练完成的RLHF模型在离线评估中表现优异，奖励模型分数很高，但上线后用户反馈其回答变得越来越"模式化"、奉承、且缺乏信息量。你认为可能的原因是什么？你会从哪些方面着手分析和解决这个问题？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗重点
**标签**:#RLHF问题诊断 #模型退化
**公司**:OpenAI、Anthropic、字节

---

#### Q10:你知道Deepseek的GRPO吗，它和PPO的主要区别是什么？优劣是什么？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#GRPO #DeepSeek
**公司**:字节、阿里(前沿技术)

---

#### Q11:GSPO和DAPO有听说过吗？他们和GRPO有什么区别？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗(前沿)
**标签**:#GSPO #DAPO #前沿算法
**公司**:字节、阿里(前沿技术)

---

#### Q12:如何解决信用分配问题？token级别和seq级别的奖励有何不同？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#信用分配 #奖励设计
**公司**:OpenAI、DeepMind

---

#### Q13:除了人类反馈，我们还可以利用AI自身的反馈来做对齐，即RLAIF。请谈谈你对RLAIF的理解，它的潜力和风险分别是什么？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#RLAIF #AI反馈
**公司**:OpenAI、Anthropic、字节(前沿)

---

### 3.2 SFT训练实践(⭐⭐⭐)

#### Q14:SFT 的 loss 如何只计算回答部分？(如何 ignore padding token?)

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#SFT #训练技巧
**公司**:美团(真题)

---

#### Q15:你对SFT的理解是什么？与预训练相比有什么差异？

**难度**:⭐⭐
**岗位**:通用
**标签**:#SFT #预训练对比
**公司**:字节、阿里

---

#### Q16:SFT冷启动时数据集构造需要注意哪些因素？为什么要做数据清洗与均衡采样？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#数据构造 #数据清洗
**公司**:字节、阿里(高频)

---

#### Q17:微调时的训练数据是怎么构建的？如何保证样本多样性和质量？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#数据构建 #样本质量
**公司**:字节、阿里(高频)

---

#### Q18:SFT+DPO训练怎么组织这部分数据的？是自己构造还是用公开数据？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#数据组织 #DPO数据
**公司**:美团、字节(真题)

---

#### Q19:SFT 的数据集是越大越好吗？会存在scaling law 吗？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#数据规模 #Scaling Law
**公司**:字节、阿里(真题)

---

#### Q20:SFT使用的数据可能和原始模型预训练时的数据分布有较大区别，怎么解决？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#数据分布 #域适应
**公司**:字节、阿里(真题)

---

#### Q21:SFT和强化学习各自有什么优缺点，分别适用于什么场景？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#SFT #RL对比
**公司**:字节、DeepSeek(真题)

---

#### Q22:什么场景下用SFT，什么场景下用RL？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#方法选择 #场景适配
**公司**:字节、阿里

---

### 3.3 强化学习进阶(⭐⭐⭐⭐)

#### Q23:PPO/GRPO 微调后，如何防止模型在分布外(OOD)问题上性能崩塌？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗重点
**标签**:#OOD #模型鲁棒性
**公司**:美团、字节(高频)

---

#### Q24:是否自己实现过 RLHF 流程？不用框架能否手写 PPO 核心逻辑？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗重点
**标签**:#RLHF实现 #编程能力
**公司**:美团、字节

---

#### Q25:为什么PPO要用value baseline和GAE？它们如何让训练更稳定？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗重点
**标签**:#PPO #训练稳定性
**公司**:DeepSeek、字节(真题)

---

#### Q26:为什么GRPO在训练MOE时会出问题？原因是啥，怎么改进策略？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗(前沿)
**标签**:#GRPO #MoE训练
**公司**:DeepSeek(真题)

---

#### Q27:GRPO的KL散度是什么？KL散度中超参数如何设计？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#GRPO #KL散度
**公司**:DeepSeek、字节(真题)

---

#### Q28:为什么使用强化学习会存在训练不稳定问题？为什么业界还在用？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#RL稳定性 #权衡取舍
**公司**:字节、阿里

---

#### Q29:rollout数量、batchsize数量和计算资源(卡的数量)有什么关系？线性？非线性？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#资源调度 #分布式RL
**公司**:字节(真题)

---

#### Q30:真实采样数量一定等于rollout数量吗？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#采样策略 #RLHF实践
**公司**:字节(真题)

---

#### Q31:交叉熵和KL散度的联系和区别？PPO的KL散度可以改成交叉熵吗？分类任务可以用KL散度吗？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#损失函数 #理论基础
**公司**:字节(真题)

---

#### Q32:在使用 GRPO 提升大模型的Function Calling 能力时，除了结果奖励(outcome reward)，还可以如何设计过程奖励(process reward)？

**难度**:⭐⭐⭐⭐
**岗位**:算法岗重点
**标签**:#奖励设计 #过程奖励
**公司**:字节(真题)

---

### 1.4 推理与优化(⭐⭐⭐)

#### Q17:如何降低 Transformer 的计算复杂度？常见的稀疏注意力变体有哪些？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#计算优化 #稀疏注意力
**公司**:字节、阿里(真题)

---

#### Q18:KV Cache是什么？为什么能极大地提升推理速度？

**难度**:⭐⭐⭐
**岗位**:通用
**标签**:#KVCache #推理优化
**公司**:字节、阿里、腾讯(高频)

---

#### Q19:LoRA微调的原理是什么？秩 r 的选择会对模型表现产生什么影响？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#LoRA #参数高效微调
**公司**:字节、阿里、美团(高频)

---

#### Q20:在有限算力下做大模型微调有哪些常用方法？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#微调策略 #资源优化
**公司**:字节、阿里

---

#### Q21:训练一个7B模型要占用多少显存？不同ZeRO阶段能节省多少显存？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#显存计算 #分布式训练
**公司**:字节、阿里(高频)

---

#### Q22:DeepSpeed ZeRO Stage 1-3的区别是什么？什么时候用FSDP会更好？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#DeepSpeed #分布式训练
**公司**:字节、阿里(高频)

---

#### Q23:vLLM框架是怎么做推理加速的？

**难度**:⭐⭐⭐
**岗位**:算法岗、开发岗
**标签**:#vLLM #推理优化
**公司**:字节、阿里(工程重点)

---

#### Q24:如果量化后模型理解能力下降怎么办？怎么做精度补偿？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#模型量化 #精度优化
**公司**:字节、腾讯

---

#### Q25:QLoRA是怎么降低资源成本的？NF4和FP16这组组合为什么有效？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#QLoRA #量化技术
**公司**:美团、字节(真题)

---

#### Q26:如何估算 LLaMA-7B 模型推理时的显存占用？

**难度**:⭐⭐⭐
**岗位**:算法岗重点
**标签**:#显存估算 #资源规划
**公司**:美团、字节(真题)

---

#### Q27:Prefix LM、Causal LM、Encoder-Decoder 三类架构的适用场景与优缺点？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#模型架构 #架构对比
**公司**:美团(真题)

---

#### Q28:bf16 和 float16 的区别？各占多少位？训练中如何选择？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#数值精度 #训练技巧
**公司**:美团、字节(真题)

---

#### Q29:Transformer为什么用 LayerNorm 而不是 BatchNorm？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#归一化 #架构设计
**公司**:美团、字节(高频)

---

#### Q30:LLM训练的时候为什么需要warmup？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#训练策略 #学习率调度
**公司**:阿里、腾讯

---

#### Q31:对比学习中的batch size是大一些好还是小一些好？为什么？

**难度**:⭐⭐
**岗位**:算法岗
**标签**:#对比学习 #训练技巧
**公司**:阿里(真题)

---

#### Q32:Tokenization 是如何工作的？BPE、WordPiece 有啥区别？

**难度**:⭐⭐
**岗位**:通用
**标签**:#Tokenization #编码
**公司**:字节、阿里(已在Q8中详细解答)

---

## 附录:学习建议

### 算法岗学习路径
1. **深入理解原理**(3-5天)
   - 手推 Attention 公式
   - 理解 Scaling Laws 数学推导
   - 掌握 PPO/DPO 损失函数

2. **论文阅读**(持续)
   - 每周1-2篇顶会论文
   - 重点:Transformer、RLHF、Scaling Laws

3. **实验验证**(可选)
   - 复现经典实验
   - 消融实验练习

### 开发岗学习路径
1. **理解核心概念**(2-3天)
   - 知道 Attention 是什么
   - 了解常见模型架构
   - 理解解码策略

2. **框架实践**(重点)
   - 熟悉 Transformers 库
   - 会使用 Tokenizer
   - 了解推理优化技巧

3. **工程应用**(重点)
   - KV Cache 优化
   - 量化部署
   - API 调用与成本控制

---

**总题目数**:56题(LLM 32题 + VLM 11题 + RLHF 13题)

**下一步**:[查看 RAG 系统题](./02-rag-questions.md) | [查看 Agent 核心题](./03-agent-questions.md)
